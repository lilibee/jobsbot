{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Job Posting Data from Indeed \n",
    "Source:\n",
    "https://towardsdatascience.com/scraping-job-posting-data-from-indeed-using-selenium-and-beautifulsoup-dfc86230baac\n",
    "\n",
    "strategy:\n",
    "\n",
    "* Get all the job posting links\n",
    "* Click each link and parse text from the job posting page \n",
    "* Store the parsed text data\n",
    "\n",
    "The questions to address\n",
    "* optimizing query strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Researching job analysis with machine learning\n",
    "\n",
    "https://www.jobspikr.com/blog/analysis-of-machine-learning-job-listings-data-reveals-the-key-skills/\n",
    "\n",
    " Further, we find that data science jobs can be grouped into three main personas: Core data scientists, researchers, and big data specialists. https://www.glassdoor.com/research/data-scientist-personas/\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get all the job posting links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# to write from list to csv file\n",
    "import csv\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# to get string punctuation constant\n",
    "import string\n",
    "from re import sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    " \"\"\"\n",
    "    Given the url of a page, this function returns the soup object.\n",
    "    \n",
    "    Parameters:\n",
    "        url: the link to get soup object for\n",
    "    \n",
    "    Returns:\n",
    "        soup: soup object\n",
    "    \"\"\"\n",
    "\n",
    "def get_soup(url):\n",
    "    driver = webdriver.Chrome(\"C:/Users/lili/Documents/icode/scraping/chromedriver\")\n",
    "    driver.get(url)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    driver.close()\n",
    "    return soup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \"\"\"\n",
    "    Grab all non-sponsored job posting links from a Indeed search result page using the given soup object\n",
    "    filter for postings with no keywords in title\n",
    "    \n",
    "    Parameters:\n",
    "        soup: the soup object corresponding to a search result page\n",
    "                e.g. https://ca.indeed.com/jobs?q=data+scientist&l=Toronto&start=20\n",
    "        key_words: the specific job title we are filtering for \n",
    "                    e.g. \"data\" and \"analyst\"\n",
    "        base_url: enalbes us to search in different indeed domains il.indeed vs. ca.indeed for example\n",
    "    \n",
    "    Returns:\n",
    "        urls: a python list of job posting urls\n",
    "    \n",
    "    \"\"\"\n",
    "def grab_job_links(soup, key_words, base_url):\n",
    "    urls = []\n",
    "    for link in soup.find_all('h2', {'class': 'jobtitle'}):\n",
    "        partial_url = link.a.get('href')\n",
    "        t = link.a.get('title').lower()\n",
    "        t = t.split()\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        t_stripped = [w.translate(table) for w in t]\n",
    "        print(t)\n",
    "        if (key_words[0] in t_stripped ) and (key_words[1] in t_stripped ):\n",
    "            print(\"yessss\")\n",
    "            url = base_url + partial_url\n",
    "            urls.append(url)\n",
    "        else:\n",
    "            continue\n",
    "    return urls\n",
    "\n",
    "#urls = grab_job_links(soup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " \"\"\"\n",
    "    Grab number of result pages, from a Indeed search result page using the given soup object\n",
    "    \n",
    "    Parameters:\n",
    "        soup: the soup object corresponding to a search result page\n",
    "                e.g. https://ca.indeed.com/jobs?q=data+scientist&l=Toronto&start=20\n",
    "    \n",
    "    Returns:\n",
    "        num_pages: integer number of result pages\n",
    "    \n",
    "    \"\"\"\n",
    "def grab_num_pages(soup):\n",
    "    page_in_search = soup.find(name='div', attrs={'id':\"searchCount\"}).get_text()\n",
    "    p = re.compile('\\d+')\n",
    "    num_pages = p.findall(page_in_search)\n",
    "    num_pages = int(num_pages[1])\n",
    "    return num_pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Get all the job posting URLs resulted from a specific search.\n",
    "    \n",
    "    Parameters:\n",
    "        query: job title to query\n",
    "        num_pages: number of pages needed\n",
    "        location: city to search in\n",
    "    \n",
    "    Returns:\n",
    "        urls: a list of job posting URL's (when num_pages valid)\n",
    "        max_pages: maximum number of pages allowed ((when num_pages invalid))\n",
    "    \"\"\"\n",
    "\n",
    "def get_urls(base_url1, query, num_pages, location):\n",
    "    # We always need the first page\n",
    "    base_url = 'https://{}.com/jobs?q={}&l={}'.format(base_url1, query, location)\n",
    "    soup = get_soup(base_url)\n",
    "    urls = grab_job_links(soup, (query.split('+')), 'indeed')\n",
    "    num_listings = grab_num_pages(soup)\n",
    "    print(\"num_listings\",num_listings)\n",
    "    num_pages_calc = int(num_listings/10)\n",
    "    print(\"num_pages_calc\",num_pages_calc)\n",
    "     \n",
    "\n",
    "    # starting page 2\n",
    "    for i in range(2, num_pages_calc+1):\n",
    "            num = (i-1) * 10\n",
    "            base_url = 'https://{}.com/jobs?q={}&l={}&start={}'.format(base_url, query, location, num)\n",
    "            try:\n",
    "                soup = get_soup(base_url)\n",
    "                # We always combine the results back to the list\n",
    "                urls += grab_job_links(soup, (query.split('+')))\n",
    "            except:\n",
    "                continue\n",
    "    return urls\n",
    "                \n",
    "\n",
    "    \n",
    "# finished stage 1 we have urls a list  with job posting links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'scientist']\n",
      "yessss\n",
      "['data', 'scientist']\n",
      "yessss\n",
      "['data', 'scientist']\n",
      "yessss\n",
      "['data', 'scientist']\n",
      "yessss\n",
      "['customer', 'experience', 'analyst', '(data', 'scientist)']\n",
      "yessss\n",
      "['statistical', 'research', 'and', 'data', 'science', 'intern']\n",
      "['data', 'scientist']\n",
      "yessss\n",
      "['data', 'scientist']\n",
      "yessss\n",
      "['data', 'scientist', '-', 'machine', 'learning']\n",
      "yessss\n",
      "['data', 'scientist']\n",
      "yessss\n",
      "num_listings 2\n",
      "num_pages_calc 0\n",
      "length list urls 1\n",
      "['practice', 'analyst', 'data', '-', 'reproductive', 'endocrinology', 'specialty']\n",
      "yessss\n",
      "['customer', 'experience', 'analyst', '(data', 'scientist)']\n",
      "yessss\n",
      "['data', 'analyst,', 'customer', 'success']\n",
      "yessss\n",
      "['data', 'analyst']\n",
      "yessss\n",
      "['data', 'analyst', '-', 'business', 'intelligence']\n",
      "yessss\n",
      "['clinical', 'data', 'standards', 'analyst']\n",
      "yessss\n",
      "['data', 'analyst,', 'customer', 'success,', 'analytics']\n",
      "yessss\n",
      "['him', 'data', 'integrity', 'and', 'quality', 'analyst']\n",
      "yessss\n",
      "['data', 'analyst']\n",
      "yessss\n",
      "['data', 'analyst']\n",
      "yessss\n",
      "num_listings 6\n",
      "num_pages_calc 0\n",
      "length list urls 2\n",
      "['data', 'engineer']\n",
      "yessss\n",
      "['data', 'engineer']\n",
      "yessss\n",
      "['engineer,', 'data', 'systems']\n",
      "yessss\n",
      "['data', 'engineer']\n",
      "yessss\n",
      "['data', 'engineer']\n",
      "yessss\n",
      "['data', 'engineer']\n",
      "yessss\n",
      "['data', 'engineer']\n",
      "yessss\n",
      "['data', 'engineer-assistant', 'vice', 'president']\n",
      "['data', 'engineer']\n",
      "yessss\n",
      "['data', 'scientist']\n",
      "num_listings 6\n",
      "num_pages_calc 0\n",
      "length list urls 3\n"
     ]
    }
   ],
   "source": [
    "query = [\"data+scientist\", 'data+analyst', 'data+engineer']\n",
    "location = [\"New+York\"]\n",
    "list_urls =[]\n",
    "\n",
    "for i in range(len(query)):\n",
    "    list_pos =[]\n",
    "    urls1 = get_urls('indeed',query[i], 5, location)\n",
    "\n",
    "    list_urls.append(urls1)\n",
    "    print('length list urls', len(list_urls))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not in data scientist\n",
    "['real', 'world', 'data', 'scientist/epidemiologist']\n",
    "['decision', 'scientist']\n",
    "['applied', 'scientist', 'intern', '-', 'alexa', 'shopping']\n",
    "['fraud', 'scientist', 'engineer']\n",
    "['applied', 'scientist']\n",
    "['bi', 'team', 'leader']\n",
    "['senior', 'r&d/data', 'scientist']\n",
    "senior', 'r&d/data', 'scientist']\n",
    "['excellent', 'excel', 'command.']\n",
    "['data', 'science', '-', 'team', 'lead']\n",
    "['excellent', 'inter-personal,', 'communication', '&', 'teamwork', 'skills']\n",
    "how come this gets to be the title\n",
    "['head', 'of', 'data', 'science']\n",
    "['data', 'product', 'manager']\n",
    "['senior', 'applied', 'scientist', '-', 'alexa', 'shopping']\n",
    "['senior', 'security', 'data-scientist']\n",
    "same problem\n",
    "['details', 'oriented,', 'efficient', 'and', 'organized,', 'able', 'to', 'meet', 'deadlines.']\n",
    "['strong', 'analytical', '&', 'technical', 'skills', '&', 'data', 'orientation.']\n",
    "['business', 'analyst']\n",
    "['director', 'of', 'data', 'science']\n",
    "['איש', 'ביג', 'דאטה', 'big', 'data', '|', 'מנהל', 'מוצר', '-', 'תוכנה']\n",
    "BI JOBS\n",
    "['taboola', 'protect', 'business', 'analyst']\n",
    "['business/data', 'analyst']\n",
    "['business', 'analyst']\n",
    "['business', 'operations', 'analyst-intern']\n",
    "['bi', 'analyst']\n",
    "\n",
    "analyst jobs\n",
    "['product', 'analyst']\n",
    "\n",
    "['junior', 'shipping', '&', 'payment', 'analyst']\n",
    "['customer', 'support', 'data', '&', 'operations', 'analyst']\n",
    "['operations', 'analyst']\n",
    "['fraud', 'analyst']\n",
    "['business', 'enablement', 'analyst']\n",
    "['junior', 'online', 'marketing', 'analyst']\n",
    "['strategy', 'analyst']\n",
    "['client', 'services', 'level', '2', 'analyst']\n",
    "['junior', 'game', 'analyst']\n",
    "['product', 'analyst']\n",
    "['product', 'data', 'scientist']\n",
    "['commission', 'analyst']\n",
    "['malware', 'analyst']\n",
    "['financial', 'analyst', '–', 'student', 'position']\n",
    "['product', 'analyst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write list of lists to csv file \n",
    "\n",
    "\n",
    "with open('urls.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(list_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n",
      "51\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "print(len(list_urls[0]))\n",
    "print(len(list_urls[1]))\n",
    "print(len(list_urls[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Click each link and parse text from the job posting page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Get the text portion including both title and job description of the job posting from a given url\n",
    "    \n",
    "    Parameters:\n",
    "        url: The job posting link\n",
    "        \n",
    "    Returns:\n",
    "        title: the job title (if \"data scientist\" is in the title)\n",
    "        posting: the job posting content    \n",
    "    \"\"\"\n",
    "\n",
    "def get_posting1(url):\n",
    "    soup = get_soup(url)\n",
    "    title = soup.find(name='h3').getText().lower()\n",
    "    posting = soup.find(name='div', attrs={'class': \"jobsearch-JobComponent\"}).get_text()\n",
    "    return title, posting.lower()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    get title + discribtion for every job posting url\n",
    "    \n",
    "    parameters: \n",
    "            url list 'urls'\n",
    "            \n",
    "    returns: \n",
    "            list with tuples job title and description\n",
    "'''\n",
    "\n",
    "posting_list =[]\n",
    "for i in range(0, len(list_urls[2])):\n",
    "    posting = get_posting1(list_urls[2][i])\n",
    "    posting_list.append(posting)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "posting_list\n"
     ]
    }
   ],
   "source": [
    "# check len posting list \n",
    "data_eng = posting_list.copy()\n",
    "print(len(posting_list))\n",
    "print('posting_list')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n",
      "51\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "print(len(data_sc))\n",
    "print(len(data_ana))\n",
    "print(len(data_eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save posting list to csv\n",
    "# so will not have to run all process again\n",
    "\n",
    "\n",
    "df = pd.DataFrame(posting_list)\n",
    "df.head()\n",
    "df.to_csv('posting_list.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get list of unique postings by keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'taboola'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    gets company name from description\n",
    "    \n",
    "    parameters: \n",
    "            listing\n",
    "            \n",
    "    returns: \n",
    "            company name\n",
    "'''\n",
    "\n",
    "\n",
    "def pick_company(position):\n",
    "    pos_com = re.compile('^[^-]+')\n",
    "    sentence_l = pos_com.findall(position[1])\n",
    "    company = sentence_l[0][len(position[0]):]\n",
    "    \n",
    "    return company\n",
    "pick_company(posting_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n",
      "51\n",
      "77\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "   filters listing list for listings with specific keywords\n",
    "    \n",
    "    parameters: \n",
    "            keywords and listings list\n",
    "            \n",
    "    returns: \n",
    "            sublist of listings which contain keywords\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def filter_listings(key_word, posting_list):\n",
    "    listx = []\n",
    "    for k in posting_list:\n",
    "        if key_word[0] in k[0] and key_word[1] in k[0]:\n",
    "            if k in listx:\n",
    "                continue\n",
    "            else:\n",
    "                listx.append(k)\n",
    "    return listx\n",
    "\n",
    "data_sci_des =filter_listings(('data','scientist'), data_sc)\n",
    "data_ana_des =filter_listings(('data','analyst'), data_ana)\n",
    "data_eng_des =filter_listings(('data','engineer'), data_eng)\n",
    "print(len(data_sci_des))\n",
    "print(len(data_ana_des))\n",
    "print(len(data_eng_des))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98  lengthlist\n",
      "viavan viavan\n",
      "100\n",
      "yeees\n",
      "seebo seebo\n",
      "100\n",
      "yeees\n",
      "viavan via\n",
      "100\n",
      "yeees\n",
      "joytunes joytunes\n",
      "100\n",
      "yeees\n",
      "optimove optimove\n",
      "100\n",
      "yeees\n",
      "intuit intuit\n",
      "100\n",
      "yeees\n",
      "seev seev\n",
      "100\n",
      "yeees\n",
      "הראל חברה לביטוח הראל חברה לביטוח\n",
      "100\n",
      "yeees\n",
      "perimeterx perimeterx\n",
      "100\n",
      "yeees\n",
      "perion perion\n",
      "100\n",
      "yeees\n",
      " \n",
      "100\n",
      "yeees\n",
      " \n",
      "100\n",
      "yeees\n",
      "apple apple\n",
      "100\n",
      "yeees\n",
      "seev seev\n",
      "100\n",
      "yeees\n",
      "98\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "   generate list of non-identical duplicates\n",
    "    \n",
    "    parameters: \n",
    "            list of postings\n",
    "            \n",
    "    returns: \n",
    "            list with duplicates only\n",
    "'''\n",
    "def list_duplicates(data_sci_des):\n",
    "    print(len(data_sci_des),\" lengthlist\")\n",
    "    duplicates = []\n",
    "    for posting in range (len(data_sci_des)):\n",
    "        looping = posting + 1\n",
    "        while looping < len(data_sci_des):\n",
    "           \n",
    "            if fuzz.token_sort_ratio(data_sci_des[posting], data_sci_des[looping]) >= 70:\n",
    "                if fuzz.token_sort_ratio(data_sci_des[posting], data_sci_des[looping]) == 100:\n",
    "                    duplicates.append(data_sci_des[posting])\n",
    "                else:\n",
    "                    if data_sci_des[posting][0] == data_sci_des[looping][0]:\n",
    "                       \n",
    "                        if fuzz.partial_ratio(pick_company(data_sci_des[posting]), pick_company(data_sci_des[looping]))>80:\n",
    "                            print(pick_company(data_sci_des[posting]), pick_company(data_sci_des[looping]))\n",
    "                            print(fuzz.partial_ratio(pick_company(data_sci_des[posting]), pick_company(data_sci_des[looping])))\n",
    "                            print(\"yeees\")\n",
    "                            duplicates.append(data_sci_des[posting])\n",
    "                        else:\n",
    "                            print(fuzz.partial_ratio(pick_company(data_sci_des[posting]), pick_company(data_sci_des[looping])))\n",
    "                            print(\"not ehough\")\n",
    "                            print(0)\n",
    "                         \n",
    "                        \n",
    "                    \n",
    "                break\n",
    "            else:\n",
    "                looping += 1\n",
    "       \n",
    "        \n",
    "    return duplicates\n",
    "        \n",
    "data_sci_des1 =  list_duplicates(data_sci_des)\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "   generate list of postings without non-identical duplicates\n",
    "    \n",
    "    parameters: \n",
    "            list of postings\n",
    "            \n",
    "    returns: \n",
    "            list of postings without non identical duplicates.\n",
    "'''\n",
    "print(len(data_sci_des1))\n",
    "\n",
    "def delete_duplicates(listing_by_keyword):\n",
    "  #  dups = list_duplicates(listing_by_keyword)\n",
    "    for i in data_sci_des1:\n",
    "        listing_by_keyword.remove(i)\n",
    "    return listing_by_keyword\n",
    "\n",
    "data_sc_nodups =delete_duplicates(data_sci_des)\n",
    "print(len(data_sc_nodups))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a - playing with nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ie_preprocess(document):\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    return sentences\n",
    "import nltk\n",
    "sentence = ie_preprocess(zzz[1][1])\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "my_text = ''\n",
    "for zz in zzz:\n",
    "    my_text += zz[1]\n",
    "#print(my_text)\n",
    "\n",
    "tokens = word_tokenize(my_text)\n",
    "print(len(tokens))\n",
    "#print(tokens)\n",
    "print('number of unique words')\n",
    "print(len(set(tokens)))\n",
    "\n",
    "filtered_words = [word.lower() for word in tokens if word not in stopwords.words('english')]\n",
    "print('no stopwords')\n",
    "print(len(filtered_words))\n",
    "words_freq = {}\n",
    "\n",
    "for word in filtered_words:\n",
    "    if word in words_freq.keys():\n",
    "        words_freq[word] += 1\n",
    "        continue\n",
    "    else:\n",
    "        words_freq[word] =1\n",
    "print(words_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract minimum qualification level\n",
    "\n",
    "def m_toar(posting):\n",
    "    ma = re.compile('(?:msc|m.sc.|master’s|תואר שני|advanced degree|masters|תואר מתקדם)')\n",
    "    phd = re.compile('(?:phd|phd.)')\n",
    "    ba = re.compile('(?:ba/|b.sc.|bsc|bachelor’s|first degree|תואר אקדמי)')\n",
    "    if len(ba.findall(posting)) > 0:\n",
    "        print('ba')\n",
    "        min_toar = 1\n",
    "    elif len(ma.findall(posting)) > 0: \n",
    "        print('ma')\n",
    "        min_toar = 2\n",
    "    elif len(phd.findall(posting)) > 0: \n",
    "        print('phd')\n",
    "        min_toar = 3\n",
    "    else:\n",
    "        min_toar = 0\n",
    "        print(posting)\n",
    "    return min_toar\n",
    "\n",
    "\n",
    "index = 0\n",
    "for zz in zzz:\n",
    "    print(index)\n",
    "    sent_bag = break_sentences(zz[1])\n",
    "    academic = thematic_ana(education,sent_bag)\n",
    "    #print(academic)\n",
    "    print()\n",
    "    toar = m_toar(zz[1])\n",
    "    print(\"the toar is!!!!!!!!!!!!\", toar)\n",
    "    index += 1       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zzz[12][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. analyze listing content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_scope =['in this job you will:','requirements']\n",
    "prog_in =['python','java','tensorflow', 'sql', 'spark', 'kafka', 'r', 'cassandra', 'elasticsearch', 'bigquery', 'google cloud', 'docker', 'scala', 'c++','matlab']\n",
    "education = ['m.sc.', 'phd.','bsc/msc','master’s', 'computer science', 'cs', 'ee', 'mathematics', 'engineering', 'physics','related','degree', 'bachelor’s','statistics']\n",
    "experience = ['years', 'experience']\n",
    "optional = ['nice to have:', '– an advantage']\n",
    "skills =['communication', 'accuracy','visualization', 'machine learning', 'deep learning']\n",
    "junior = ['student']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "   generate list of composite sentenses in posting\n",
    "    \n",
    "    parameters: \n",
    "            one posting description string\n",
    "            \n",
    "    returns: \n",
    "            list of sentences in posting\n",
    "'''\n",
    "\n",
    "# analyse sentenses in listing\n",
    "\n",
    "def break_sentences(listing):\n",
    "\n",
    "    length_listing = len(listing)\n",
    "    sentence_l = []\n",
    "    sentence = re.compile('[^\\n]+\\n')\n",
    "    end_sentence = re.compile('[\\n](.+)\\-.+ago')\n",
    "    # the last part of the listing does not have line break in the end therefore needs special regex\n",
    "    \n",
    "    sentence_l = sentence.findall(listing)\n",
    "    end_sentence_l = end_sentence.findall(listing)\n",
    "    if len(sentence_l)> 0:   \n",
    "        sentence_l.append(end_sentence_l[0])\n",
    "   \n",
    "   \n",
    "    len_sentences = 0\n",
    "    for i in sentence_l:\n",
    "        len_sentences += len(i)\n",
    "    print(\"listing\", length_listing, \"sentences\", len_sentences)\n",
    "    if len_sentences/length_listing < 0.9:\n",
    "        sentence_l = sent_tokenize(listing)\n",
    "        print(\"tokenize\")\n",
    "        for i in sentence_l:\n",
    "            len_sentences += len(i)\n",
    "        print(len_sentences)\n",
    "    return sentence_l\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "   filters the relevant sentence by theme\n",
    "    \n",
    "    parameters: \n",
    "            theme( eg. education qualification, experience, etc.) and posting\n",
    "            \n",
    "    returns: \n",
    "            list of relevant sentences\n",
    "'''\n",
    "# problem this kind of cleaning breaks relevant expresions such as \"computer science\"\n",
    "\n",
    "def thematic_ana(theme, listing):\n",
    "  \n",
    "    temp_words = []\n",
    "    selected = []\n",
    "    for i in listing:\n",
    "        i = i.replace('/',' ')\n",
    "        words = i.split()\n",
    "        table = str.maketrans('', '', sub('\\+', '',string.punctuation))\n",
    "        stripped = [w.translate(table) for w in words]\n",
    "        for ii in stripped:\n",
    "            if ii in theme:\n",
    "                selected.append(stripped)\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "    return selected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "for zz in zzz:\n",
    "    print(index)\n",
    "    print(zz)\n",
    "    print('')\n",
    "    sent_bag = break_sentences(zz[1])\n",
    "    prog_lang = thematic_ana(prog_in,sent_bag)\n",
    "    print(prog_lang)\n",
    "    print()\n",
    "    academic = thematic_ana(education,sent_bag)\n",
    "    print(academic)\n",
    "    print()\n",
    "    exp = thematic_ana(experience,sent_bag)\n",
    "    print(exp)\n",
    "    print()\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zzz[3])\n",
    "print()\n",
    "bagg = break_sentences(zzz[3][1])\n",
    "temp_words = []\n",
    "selected = []\n",
    "for i in bagg:\n",
    "    print(i)\n",
    "    temp_words = i.split()\n",
    "    for word in temp_words:\n",
    "        word2 = word.replace('/',' ')\n",
    "        if word2.strip(',') in prog_in:\n",
    "            print(word2)\n",
    "            selected.append(i)\n",
    "            break\n",
    "        else:\n",
    "            print(\"+++++++++++++++++\", word2)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take posting list and count relevant vs. irrelevant\n",
    "############## how important it is to count relevant vs irrele\n",
    "\n",
    "data_scientist_jobs = defaultdict(list)\n",
    "#data_analyst_jobs = defaultdict(list)\n",
    "data_sci_des = []\n",
    "data_eng_des = []\n",
    "data_ana_des = []\n",
    "datasci_related = [] \n",
    "non_data_sc = []\n",
    "\n",
    "\n",
    "for k in posting_list:\n",
    "    if 'data' in k[0] and (('scientist' in k[0]) or ('science' in k[0])):\n",
    "        data_sci_des.append(k) \n",
    "        if k[0] in data_scientist_jobs.keys():\n",
    "            data_scientist_jobs[k[0]] += 1\n",
    "        else:\n",
    "            data_scientist_jobs[k[0]] = 1\n",
    "    elif 'data' in k[0] and 'engineer' in k[0]:\n",
    "        data_eng_des.append(k) \n",
    "        if k[0] in data_scientist_jobs.keys():\n",
    "            data_scientist_jobs[k[0]] += 1\n",
    "        else:\n",
    "            data_scientist_jobs[k[0]] = 1\n",
    "    elif 'data' in k[0] and 'analyst' in k[0]:\n",
    "        data_ana_des.append(k) \n",
    "        if k[0] in data_scientist_jobs.keys():\n",
    "            data_scientist_jobs[k[0]] += 1\n",
    "        else:\n",
    "            data_scientist_jobs[k[0]] = 1\n",
    "    elif 'data scientist' in k[1]:\n",
    "        datasci_related.append(k) \n",
    "        \n",
    "    else:\n",
    "        if k in  non_data_sc:\n",
    "            continue\n",
    "        else:\n",
    "            non_data_sc.append(k) \n",
    "        print(k)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('data science posting', len(data_sci_des))\n",
    "print('data engineer posting', len(data_eng_des))\n",
    "print('data analyst posting', len(data_ana_des))\n",
    "print('datasci_related', len(datasci_related))\n",
    "print('non data science', len(non_data_sc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h in range(len(non_data_sc)):\n",
    "    print(h)\n",
    "    print(non_data_sc[h])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentence_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping code:\n",
    "for city in city_set:\n",
    "    for start in range(0, max_results_per_city, 10):\n",
    "        page = requests.get(\"https://il.indeed.com/jobs?q=data+scientist&l=tel+aviv&start=\" + str(start))\n",
    "        time.sleep(1)  #ensuring at least 1 second between page grabs\n",
    "        soup = BeautifulSoup(page.text, \"lxml\", from_encoding=\"utf-8\")\n",
    "        for div in soup.find_all(name=\"div\", attrs={\"class\":\"row\"}): \n",
    "            #specifying row num for index of job posting in dataframe\n",
    "            #num = (len(sample_df) + 1)\n",
    "            # think this shouldnt be +1\n",
    "            num = (len(sample_df))\n",
    "            #creating an empty list to hold the data for each posting\n",
    "            job_post = [] \n",
    "            #append city name\n",
    "            job_post.append(city) \n",
    "            #grabbing job title\n",
    "            for a in div.find_all(name=\"a\", attrs={\"data-tn-element\":\"jobTitle\"}):\n",
    "   \n",
    "                job_post.append(a[\"title\"]) \n",
    "            #grabbing company name\n",
    "            company = div.find_all(name=\"span\", attrs={\"class\":\"company\"})\n",
    "            if len(company) > 0:\n",
    "                for b in company:\n",
    "                    job_post.append(b.text.strip()) \n",
    "            else:\n",
    "                sec_try = div.find_all(name=\"span\", attrs={\"class\":\"result-link-source\"})\n",
    "                for span in sec_try:\n",
    "                    job_post.append(span.text) \n",
    "            print(\"this is row 1!!!!\")\n",
    "            print(job_post)\n",
    "            #grabbing location name\n",
    "          #  for div in soup.find_all(name=\"div\", attrs={\"class\":\"companyInfoWrapper\"}): \n",
    "        \n",
    "                \n",
    "            c = soup.findAll(\"span\", attrs={'class': \"location\"})\n",
    "            for span in c:\n",
    "                temp1 = 0\n",
    "                temp1 = span.text\n",
    "                job_post.append(span.text) \n",
    "                print()\n",
    "                if len(temp1) > 0:\n",
    "                    break\n",
    "           \n",
    "            #grabbing summary text\n",
    "            d = soup.findAll(\"span\", attrs={\"class\": \"summary\"})\n",
    "            for span in d:\n",
    "                temp2 = 0\n",
    "                temp2 = span.text\n",
    "                job_post.append(span.text.strip())\n",
    "                if len(temp2) > 0:\n",
    "                    break\n",
    "           \n",
    "            \n",
    "            print(\"this is row 2!!!!\")\n",
    "            print(job_post)\n",
    "            #appending list of job post info to dataframe at index num\n",
    "            sample_df.iloc[num,:] = job_post\n",
    "#saving sample_df as a local csv file — define your own local path to save contents \n",
    "#sample_df.to_csv(\"C:\\Users\\lili\\Documents\\icode\\scraping\\ver1.csv\", encoding=\"utf-8\")\n",
    "print(sample_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# source\n",
    "* [Web Scraping Job Postings from Indeed](https://medium.com/@msalmon00/web-scraping-job-postings-from-indeed-96bd588dcb4b)\n",
    "* [Scraping Job Posting Data from Indeed using Selenium and BeautifulSoup](https://towardsdatascience.com/scraping-job-posting-data-from-indeed-using-selenium-and-beautifulsoup-dfc86230baac)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
